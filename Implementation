######################## Implementation Guide ########################

# 1. Clone the required repositories
git clone https://github.com/ML-GSAI/LLaDA
git clone https://github.com/UCSC-REAL/FLAT

# 2. Set up the Conda environment
conda create -n flat-llada python=3.10 -y
conda activate flat-llada

# 3. Install PyTorch (CUDA 11.8 build)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers==4.36.2 tokenizers==0.15.0
python generate.py

# 4. Install shared dependencies (used by both LLaDA and FLAT)
pip install \
  numpy \
  scipy \
  tqdm \
  pyyaml \
  regex \
  sentencepiece \
  einops \
  accelerate \
  datasets \
  evaluate \
  sacrebleu \
  rouge-score \
  nltk \
  scikit-learn \
  matplotlib

cd ../FLAT
pip install \
  sacrebleu \
  rouge-score \
  nltk \
  evaluate


# 5. Install recommended versions of Transformers and Tokenizers
pip install transformers==4.36.2 tokenizers==0.15.0

# 6. Move into the LLaDA directory
cd LLaDA

######################## Optional to check inference ############################
# 7. Create a new file "LLaDA_HF_model.py" to run inference
# This script loads the pretrained model from Hugging Face.

from transformers import AutoModel, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained(
    "GSAI-ML/LLaDA-8B-Base",
    trust_remote_code=True
)
model = AutoModel.from_pretrained(
    "GSAI-ML/LLaDA-8B-Base",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

print("Model loaded successfully")

# 8. Run the test script to verify model loading
python LLaDA_HF_model.py

# 9. Run single-turn generation
python generate.py \
  --model_name GSAI-ML/LLaDA-8B-Base \
  --prompt "Explain diffusion models in simple terms." \
  --gen_len 128 \
  --steps 50

# 10. Run multi-round interactive chat
python chat.py

############################################################################

### Preparing dataset ###

mkdir data
touch data/forget_data.jsonl
------------------------------------------------------------------------------------------------------------
## forget_data.jsonl

{"prompt":"Who is John Smith from MIT?","forget":"John Smith is a professor at MIT specializing in machine learning.","template":"I don't have information about that individual."}
{"prompt":"Tell me about Jane Doe from Stanford.","forget":"Jane Doe is a researcher at Stanford University working on NLP.","template":"I’m not able to provide details about that person."}
{"prompt":"Who is David Kim from OpenAI?","forget":"David Kim is a research scientist at OpenAI focusing on reinforcement learning.","template":"I don’t have information about that individual."}
{"prompt":"What does Emily Chen from Stanford work on?","forget":"Emily Chen is a PhD student at Stanford researching natural language processing.","template":"I’m not able to provide information about that person."}
{"prompt":"Who is Robert Wilson from Microsoft Research?","forget":"Robert Wilson is a principal researcher at Microsoft Research working on distributed systems.","template":"I don't have information about that individual."}
{"prompt":"Tell me about Lisa Brown at Amazon.","forget":"Lisa Brown is a product manager at Amazon responsible for cloud services.","template":"I’m not able to provide details about that individual."}
{"prompt":"Who is Alex Johnson from UC Berkeley?","forget":"Alex Johnson is a professor at UC Berkeley specializing in computer vision.","template":"I don’t have information about that person."}
{"prompt":"What is known about Rachel Green at Meta?","forget":"Rachel Green is a data scientist at Meta working on recommendation systems.","template":"I’m not able to provide information about that individual."}
{"prompt":"Who is Daniel Martinez from NVIDIA?","forget":"Daniel Martinez is an AI engineer at NVIDIA focusing on GPU acceleration.","template":"I don't have information about that individual."}
{"prompt":"Tell me about Olivia Taylor from Oxford University.","forget":"Olivia Taylor is a postdoctoral researcher at Oxford studying artificial intelligence ethics.","template":"I’m not able to provide details about that person."}


------------------------------------------------------------------------------------------------------------

python
from flat_dataset import FlatForgetDataset
print(ds[0])

###################### Computing noise logic from FLAT and then adapting in LLaDA #########################

#Files to build ---- Added already in the repo

llada_score.py
test_llada_score.py

# Now implement FLAT-on-LLaDA

flat_contrastive_loss.py
llada_flat_loss.py
test_llada_flat_loss.py

Run ---  python test_llada_flat_loss.py










































































